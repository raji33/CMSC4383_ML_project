{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataset and data cleaning to merge data from various datasets into single unified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/5cz0p2jn0zv3_v2zb1n9g_nh0000gn/T/ipykernel_14985/482546429.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price_data_clean['adj_price'] = price_data_clean['adj_price'].astype(float)\n",
      "/var/folders/vs/5cz0p2jn0zv3_v2zb1n9g_nh0000gn/T/ipykernel_14985/482546429.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price_data_clean['adj_value'] = price_data_clean['adj_value'].astype(float)\n",
      "/var/folders/vs/5cz0p2jn0zv3_v2zb1n9g_nh0000gn/T/ipykernel_14985/482546429.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price_data_clean['next_quarter'] = price_data_clean['next_quarter'].astype(float)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interest</th>\n",
       "      <th>vaccancy</th>\n",
       "      <th>adj_price</th>\n",
       "      <th>adj_value</th>\n",
       "      <th>next_quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>747.000000</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>747.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.180750</td>\n",
       "      <td>7.882329</td>\n",
       "      <td>91929.621839</td>\n",
       "      <td>85955.680745</td>\n",
       "      <td>91585.281447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.820783</td>\n",
       "      <td>1.560739</td>\n",
       "      <td>10492.184365</td>\n",
       "      <td>10287.494750</td>\n",
       "      <td>10146.662927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.650000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>62248.590821</td>\n",
       "      <td>66602.818831</td>\n",
       "      <td>62248.590821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.635000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>84988.264243</td>\n",
       "      <td>78293.570231</td>\n",
       "      <td>84988.264243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.080000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>92446.164861</td>\n",
       "      <td>86512.591473</td>\n",
       "      <td>92286.069883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.615000</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>98425.285613</td>\n",
       "      <td>92042.996096</td>\n",
       "      <td>97904.139387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.700000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>119191.182657</td>\n",
       "      <td>106048.474106</td>\n",
       "      <td>118889.752618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         interest    vaccancy      adj_price      adj_value   next_quarter\n",
       "count  747.000000  747.000000     747.000000     747.000000     747.000000\n",
       "mean     4.180750    7.882329   91929.621839   85955.680745   91585.281447\n",
       "std      0.820783    1.560739   10492.184365   10287.494750   10146.662927\n",
       "min      2.650000    5.600000   62248.590821   66602.818831   62248.590821\n",
       "25%      3.635000    6.800000   84988.264243   78293.570231   84988.264243\n",
       "50%      4.080000    7.300000   92446.164861   86512.591473   92286.069883\n",
       "75%      4.615000    9.300000   98425.285613   92042.996096   97904.139387\n",
       "max      6.700000   11.100000  119191.182657  106048.474106  118889.752618"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import matplotlib\n",
    "\n",
    "#file names for mortgages, rental vaccancy, and inflation\n",
    "fed_files = [\"../datasets/MORTGAGE30US.csv\", \"../datasets/RRVRUSQ156N.csv\", \"../datasets/CPIAUCSL.csv\" ]\n",
    "\n",
    "#read them in using list comp\n",
    "#tell pandas to parse dates into dateTime\n",
    "#use first column for row indicies\n",
    "fed_dfs = [pd.read_csv(f, parse_dates=True, index_col=0) for f in fed_files]\n",
    "\n",
    "#mergining all data into one big dataframe\n",
    "fed_data = pd.concat(fed_dfs, axis=1)\n",
    "\n",
    "#need to fix the timelines of all data to be on same timeline (weekly, monthly , biweekly)\n",
    " \n",
    " #will fill data up - so rental vaccancy monthly so assume for all weeks in month its the same value -- do same for other data\n",
    "#the ffill does a forward fill for all values missing it takes previous value and applies it forward\n",
    "fed_data = fed_data.ffill()\n",
    "\n",
    "#first value has medium sales prices for houses each week, second is zillow computed house index indicating how much it thinks average house value is worth \n",
    "zillow_files = [\"../datasets/Metro_median_sale_price_uc_sfrcondo_week.csv\", \"../datasets/Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_month.csv\"]\n",
    "\n",
    "zillow_dfs = [pd.read_csv(f) for f in zillow_files]\n",
    "\n",
    "#dataframe takes info for specific regions\n",
    "#if we doing entire us take [0] row otherwise you can go find regions like chicago [3], dallas, austin etc\n",
    "#reformat data to be each row as week and column is price it was sold --- changed first 3 to 0 to get chicago data\n",
    "zillow_dfs = [pd.DataFrame(df.iloc[3,5:]) for df in zillow_dfs]\n",
    "\n",
    "\n",
    "#combine house price (weekly) and house value (monthly) dataset together \n",
    "#converts \n",
    "for df in zillow_dfs:\n",
    "    #convert string to datatime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    #takes dates and creates month column to combine dataframes\n",
    "    df[\"month\"] = df.index.to_period(\"M\")\n",
    "\n",
    "price_data = zillow_dfs[0].merge(zillow_dfs[1], on=\"month\")\n",
    "price_data.index = zillow_dfs[0].index\n",
    "\n",
    "del price_data[\"month\"]\n",
    "price_data.columns = [\"price\", \"value\"]\n",
    "\n",
    "fed_data.index = fed_data.index + timedelta(days=2)\n",
    "\n",
    "price_data = fed_data.merge(price_data, left_index=True, right_index=True)\n",
    "\n",
    "#rename columns\n",
    "price_data.columns = [\"interest\", \"vaccancy\", \"cpi\", \"price\", \"value\"]\n",
    "\n",
    "#adj price is taking inflation out of the house value -- it takes into account only the underlying value of house change over time (removing inflation from the change)\n",
    "price_data[\"adj_price\"] = price_data[\"price\"] / price_data[\"cpi\"] * 100\n",
    "\n",
    "#adj zillow value for inflation\n",
    "price_data[\"adj_value\"] = price_data[\"value\"] / price_data[\"cpi\"] * 100\n",
    "\n",
    "#want to predict what will happen to house prices in next 3 months\n",
    "#the shift grabs the adjusted price from 13 weeks into the future and sets that value as the next quarter for the price 3 months prior\n",
    "price_data[\"next_quarter\"] = price_data[\"adj_price\"].shift(-13)\n",
    "\n",
    "# we cant use the last 13 rows for training data becuase we dont have value for next quarter so we want to remove those rows\n",
    "price_data.dropna(inplace=True)\n",
    "\n",
    "#change will be target column because we want to predict 3 months in advance house price\n",
    "#will be 1 if true (price increase) and 0 if false (price decreased or didnt change)\n",
    "price_data[\"change\"] = (price_data[\"next_quarter\"] > price_data[\"adj_price\"]).astype(int)\n",
    "\n",
    "columns_keep = [\"interest\", \"vaccancy\", \"adj_price\", \"adj_value\", \"next_quarter\"]\n",
    "\n",
    "\n",
    "price_data_clean = price_data[columns_keep]\n",
    "\n",
    "# convert to floats so the descibe function gives proper analysis of data\n",
    "price_data_clean['adj_price'] = price_data_clean['adj_price'].astype(float)\n",
    "price_data_clean['adj_value'] = price_data_clean['adj_value'].astype(float)\n",
    "price_data_clean['next_quarter'] = price_data_clean['next_quarter'].astype(float)\n",
    "\n",
    "price_data_clean.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing / transformation\n",
    "\n",
    "  - normalize intrest scale between 0-1\n",
    "  - normalize vaccancy between 0-1\n",
    "  - change the date from year-month-date to columns with year, day, and month \n",
    "  - keeping adj price and adj value and next quarter because thats what the output should be in \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m      3\u001b[0m price_data_clean[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m price_data_clean\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39myear\n\u001b[1;32m      4\u001b[0m price_data_clean[\u001b[39m'\u001b[39m\u001b[39mmonth\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m price_data_clean\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mmonth\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "price_data_clean['year'] = price_data_clean.index.year\n",
    "price_data_clean['month'] = price_data_clean.index.month\n",
    "price_data_clean['day'] = price_data_clean.index.day\n",
    "\n",
    "#use min-max scaler to normalize data as lowest value is min and highest value is max and evrything inbetween is scaled accordingly\n",
    "scaler = MinMaxScaler()\n",
    "price_data_clean[['interest', 'vaccancy', 'year', 'month', 'day']] = scaler.fit_transform(price_data_clean[['interest', 'vaccancy','year', 'month', 'day']])\n",
    "\n",
    "# remove current dataset index \n",
    "price_data_clean = price_data_clean.reset_index(drop=True)\n",
    "price_data_clean\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m predictors \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39minterest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvaccancy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39madj_price\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39madj_value\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39myear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmonth\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mday\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[39m#if want to predict actual next_quarter value use next_quarter as target value\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "predictors = [\"interest\", \"vaccancy\", \"adj_price\", \"adj_value\", \"year\", \"month\", \"day\"]\n",
    "#if want to predict actual next_quarter value use next_quarter as target value\n",
    "target = [\"next_quarter\"]\n",
    "\n",
    "\n",
    "# we are using a backtest function that will split on multiple train and validation sets. So we are just spliting dataset into 80% data in training and 20% test\n",
    "train_idx = np.arange(0, int(price_data_clean.shape[0] * 0.8))\n",
    "test_idx = np.arange(int(price_data_clean.shape[0] * 0.8), price_data_clean.shape[0])\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "train_data = price_data_clean.iloc[train_idx]\n",
    "\n",
    "\n",
    "test_data = price_data_clean.iloc[test_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analysis of the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "\n",
    "print(train_data.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the models (sklearn models need numpy arrays input)\n",
    "  - linear regression\n",
    "  - KNN\n",
    "  - neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training functions initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for predict and backtest for models\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "predictors = [\"interest\", \"vaccancy\", \"adj_price\", \"adj_value\", \"year\", \"month\", \"day\"]\n",
    "#if want to predict actual next_quarter value use next_quarter as target value\n",
    "target = [\"next_quarter\"]\n",
    "\n",
    "# are grabbing data from 2008-2013 years in advance and then predicting based on the 2014 year. Then we step a year and do 20008-2014 and predict 2015.\n",
    "#260 == 5 years of data\n",
    "START = 260\n",
    "#52 weeks in year\n",
    "STEP = 52\n",
    "\n",
    "def predict(model, train, test, predictors, target):\n",
    "    start_time = time.time()\n",
    "    model.fit(train[predictors], train[target])\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    preds = model.predict(test[predictors])\n",
    "    return preds, training_time\n",
    "\n",
    "\n",
    "# cross validaiton shouldnt be used as you dont use future to predict the past -- but it gives false sense of model working well\n",
    "#will do backtesting \n",
    "#lets us generate predictions for most data and respect the idea of using past data to predict future and not vice versa \n",
    "def backtest(model, data, predictors, target, param_grid = None):\n",
    "    all_preds=[]\n",
    "    training_times = []\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    for i in range(START, data.shape[0], STEP):\n",
    "        train = data.iloc[:i]\n",
    "        test = data.iloc[i: (i+STEP)]\n",
    "        train_pred, train_time = predict(model,train,train,predictors,target)\n",
    "        val_pred, val_time = predict(model,train,test,predictors,target)\n",
    "        all_preds.append(val_pred)\n",
    "        training_times.append(train_time)\n",
    "        train_errors.append(mean_squared_error(train[target], train_pred))\n",
    "        val_errors.append(mean_squared_error(test[target], val_pred))\n",
    "\n",
    "    preds = np.concatenate(all_preds)\n",
    "\n",
    "    #uncomment code if youd like to see what predictions model is making\n",
    "    # print(preds)\n",
    "    \n",
    "    training_time = np.mean(training_times)\n",
    "\n",
    "    mse = mean_squared_error(data.iloc[START:][target], preds)\n",
    "    mae = mean_absolute_error(data.iloc[START:][target], preds) # amount of dollars off from prediction\n",
    "    r2 = r2_score(data.iloc[START:][target], preds)\n",
    "\n",
    "    # plot of this model against the features - compare predict (y axis) to each input (scatterplots)\n",
    "    for p in predictors:\n",
    "        plt.figure()\n",
    "        plt.scatter(data.iloc[START:][p], data.iloc[START:][target])\n",
    "        plt.plot(data.iloc[START:][p], preds, color='red')\n",
    "        plt.xlabel(p)\n",
    "        plt.ylabel(target)\n",
    "        plt.title(f\"{p} vs. {target}\")\n",
    "\n",
    "    #compute avg error in dollars from prediction to actual \n",
    "    #generate loss plot - indicates how much model really learned \n",
    "    plt.figure()\n",
    "    plt.plot(train_errors, label='Training Error')\n",
    "    plt.plot(val_errors, label='Validation Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Samples')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.legend()\n",
    "\n",
    "    print(\"MSE: \", mse)\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"R-squared: \", r2) # want as close to 1 as possible - indicates \n",
    "    print(f\"Training time: {training_time}\")\n",
    "\n",
    "    if param_grid is not None:\n",
    "        # gridsearch over hyperparameters (loop over different parameters to see which preforms best)\n",
    "        best_params = None\n",
    "        best_score = float('-inf')\n",
    "        best_mse = float('inf')\n",
    "        best_mae = float('inf')\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            model.set_params(**params)\n",
    "            model.fit(train[predictors], train[target])\n",
    "            preds = model.predict(test[predictors])\n",
    "            mse = mean_squared_error(test[target], preds)\n",
    "            mae = mean_absolute_error(test[target], preds)\n",
    "            score = r2_score(test[target], preds)\n",
    "            if score > best_score:\n",
    "                best_mse = mse\n",
    "                best_mae = mae\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "\n",
    "        print(\"Best parameters:\", best_params)\n",
    "        print(\"Best R-squared:\", best_score)\n",
    "        print(\"Best MSE:\", best_mse)\n",
    "        print(\"Best MAE:\", best_mae)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression training hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression \n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sys\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "param_grid = {'copy_X': [True, False], 'fit_intercept': [True, False], 'n_jobs': [-1, None], 'positive': [True, False]}\n",
    "\n",
    "backtest(lr_model, train_data, predictors, target, param_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN model training and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import time\n",
    "import sys\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=10)\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [5, 10, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "backtest(knn_model, train_data, predictors, target, param_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network model training and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "nn_model = Sequential()\n",
    "\n",
    "# Add layers to the neural network - inputshape is 7 as 7 input columns\n",
    "nn_model.add(Dense(64, activation='relu', input_shape=(7,)))\n",
    "nn_model.add(Dense(1))\n",
    "nn_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "backtest(nn_model, train_data, predictors, target)\n",
    "\n",
    "# Define hyperparameters to test\n",
    "param_grid = {\n",
    "    'num_hidden_layers': [1, 2, 3],\n",
    "    'hidden_layer_size': [32, 64, 128],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh']\n",
    "}\n",
    "\n",
    "# Define base model\n",
    "def create_model(num_hidden_layers=1, hidden_layer_size=64, activation='relu'):\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Dense(hidden_layer_size, activation=activation, input_shape=(7,)))\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        nn_model.add(Dense(hidden_layer_size, activation=activation))\n",
    "    nn_model.add(Dense(1))\n",
    "    nn_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    return nn_model\n",
    "\n",
    "# Loop through hyperparameters and evaluate model for each set of hyperparameters\n",
    "best_score = float('-inf')\n",
    "\n",
    "for num_hidden_layers in param_grid['num_hidden_layers']:\n",
    "    for hidden_layer_size in param_grid['hidden_layer_size']:\n",
    "        for activation in param_grid['activation']:\n",
    "\n",
    "            #create model \n",
    "            nn_model = create_model(num_hidden_layers=num_hidden_layers, hidden_layer_size=hidden_layer_size, activation=activation)\n",
    "\n",
    "            all_preds=[]\n",
    "            training_times = []\n",
    "            for i in range(START, train_data.shape[0], STEP):\n",
    "                train = train_data.iloc[:i]\n",
    "                test = train_data.iloc[i: (i+STEP)]\n",
    "                response = predict(nn_model,train,test,predictors,target)\n",
    "                all_preds.append(response[0])\n",
    "                training_times.append(response[1])\n",
    "\n",
    "            preds = np.concatenate(all_preds)\n",
    "            training_time = np.mean(training_times)\n",
    "\n",
    "            mse = mean_squared_error(train_data.iloc[START:][target], preds)\n",
    "            mae = mean_absolute_error(train_data.iloc[START:][target], preds) # amount of dollars off from prediction\n",
    "            score = r2_score(train_data.iloc[START:][target], preds)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_mse = mse\n",
    "                best_mae = mae\n",
    "                best_params = {'num_hidden_layers': num_hidden_layers, 'hidden_layer_size': hidden_layer_size, 'activation': activation}\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best R-squared:\", best_score)\n",
    "print(\"Best MSE:\", best_mse)\n",
    "print(\"Best MAE:\", best_mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model Testing\n",
    "\n",
    "- final test based on the test set the model hasnt seen yet\n",
    "- will initialize the models with the best hyperparameters and train like normal and then will test on the final set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "#linear regression \n",
    "best_lr_model = LinearRegression()\n",
    "best_lr_model.set_params(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)\n",
    "\n",
    "#training best lr model\n",
    "for i in range(START, train_data.shape[0], STEP):\n",
    "    train = train_data.iloc[:i]\n",
    "    test = train_data.iloc[i: (i+STEP)]\n",
    "    response = predict(best_lr_model,train,test,predictors,target)\n",
    "\n",
    "#get prediction for test from best lr model\n",
    "preds = best_lr_model.predict(test_data[predictors])\n",
    "mse = mean_squared_error(test_data.iloc[0:][target], preds)\n",
    "mae = mean_absolute_error(test_data.iloc[0:][target], preds) # amount of dollars off from prediction\n",
    "r2 = r2_score(test_data.iloc[0:][target], preds)\n",
    "\n",
    "# get size of model \n",
    "joblib.dump(best_lr_model, 'best_lr_model.joblib')\n",
    "best_lr_model_size = os.path.getsize('best_lr_model.joblib')\n",
    "print(\"Metrics from best Linear regression model\")\n",
    "print(\"MSE: \", mse)\n",
    "print(\"MAE: \", mae)\n",
    "print(\"R-squared: \", r2) # want as close to 1 as possible - indicates \n",
    "print(\"Model size:\", best_lr_model_size, \"bytes\")\n",
    "print()\n",
    "\n",
    "\n",
    "#knn\n",
    "best_knn_model = KNeighborsRegressor(n_neighbors=10)\n",
    "best_knn_model.set_params(n_neighbors=15, weights='uniform', algorithm='auto', leaf_size=10, p=1, metric='euclidean')\n",
    "\n",
    "#train model using backtesting\n",
    "for i in range(START, train_data.shape[0], STEP):\n",
    "    train = train_data.iloc[:i]\n",
    "    test = train_data.iloc[i: (i+STEP)]\n",
    "    response = predict(best_knn_model,train,test,predictors,target)\n",
    "\n",
    "#get prediction for test\n",
    "preds = best_knn_model.predict(test_data[predictors])\n",
    "mse = mean_squared_error(test_data.iloc[0:][target], preds)\n",
    "mae = mean_absolute_error(test_data.iloc[0:][target], preds) # amount of dollars off from prediction\n",
    "r2 = r2_score(test_data.iloc[0:][target], preds)\n",
    "\n",
    "# get size of model \n",
    "joblib.dump(best_knn_model, 'best_knn_model.joblib')\n",
    "best_knn_model_size = os.path.getsize('best_knn_model.joblib')\n",
    "print(\"Metrics from best KNN model\")\n",
    "print(\"MSE: \", mse)\n",
    "print(\"MAE: \", mae)\n",
    "print(\"R-squared: \", r2) # want as close to 1 as possible - indicates \n",
    "print(\"Model size:\", best_knn_model_size, \"bytes\")\n",
    "print()\n",
    "\n",
    "# neural network model using best hyper parameters\n",
    "best_hidden_layer_size = 128\n",
    "best_num_hidden_layers = 3\n",
    "best_activation = 'relu'\n",
    "best_nn_model = Sequential()\n",
    "best_nn_model.add(Dense(best_hidden_layer_size, activation=best_activation, input_shape=(7,)))\n",
    "for i in range(best_num_hidden_layers - 1):\n",
    "  best_nn_model.add(Dense(best_hidden_layer_size, activation=best_activation))\n",
    "  best_nn_model.add(Dense(1))\n",
    "  best_nn_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#train model using backtesting for neural network\n",
    "for i in range(START, train_data.shape[0], STEP):\n",
    "    train = train_data.iloc[:i]\n",
    "    test = train_data.iloc[i: (i+STEP)]\n",
    "    response = predict(best_nn_model,train,test,predictors,target)\n",
    "\n",
    "#get prediction for test\n",
    "preds = best_nn_model.predict(test_data[predictors])\n",
    "mse = mean_squared_error(test_data.iloc[0:][target], preds)\n",
    "mae = mean_absolute_error(test_data.iloc[0:][target], preds) # amount of dollars off from prediction\n",
    "r2 = r2_score(test_data.iloc[0:][target], preds)\n",
    "\n",
    "# get size of model \n",
    "joblib.dump(best_nn_model, 'best_nn_model.joblib')\n",
    "best_nn_model_size = os.path.getsize('best_nn_model.joblib')\n",
    "print(\"Metrics from best neural network model\")\n",
    "print(\"MSE: \", mse)\n",
    "print(\"MAE: \", mae)\n",
    "print(\"R-squared: \", r2) # want as close to 1 as possible - indicates \n",
    "print(\"Model size:\", best_nn_model_size, \"bytes\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
